# -*- coding: utf-8 -*-
"""Embedded Project_ES10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vbF8HQ2GTiBlD9WpDVrv6V6jaCYXVGas
"""

! pip install kaggle
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
! kaggle datasets download -d bulentsiyah/semantic-drone-dataset
# Installing Kaggle package, set up API credentials, and download the "semantic-drone-dataset" from Kaggle.

!unzip /content/semantic-drone-dataset.zip

# Importing necessary libraries
import cv2
import numpy as np
import os
import pandas as pd

from tqdm import tqdm
from glob import glob
from albumentations import RandomCrop, HorizontalFlip, VerticalFlip

from sklearn.model_selection import train_test_split
from PIL import Image

from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, UpSampling2D, Concatenate
from tensorflow.keras.models import Model
from keras.utils import plot_model
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, Callback

# Data Augmentation
# Original images have a resolution of 6000x4000px. We've chosen to resize them to 1536x1024px while maintaining the original aspect ratio.

# Function to create directory
def create_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)

# Function to augment images and masks and save them to specified directory
def augment_data(images, masks, save_path, augment=True):
    H = 1024
    W = 1536

    # Iterating through each pair of original image and mask
    for x,y in tqdm(zip(images, masks), total=len(images)):
        print("x:", x)
        print("y:", y)
        name = x.split("/")[-1].split(".")
        print("name:", name)
        image_name = name[0]
        image_extn = name[1]

        name = y.split("/")[-1].split(".")
        mask_name = name[0]
        mask_extn = name[1]

        # Reading and resizing original image
        x = cv2.imread(x, cv2.IMREAD_COLOR)
        x = cv2.resize(x, (W, H))
        y = cv2.imread(y, cv2.IMREAD_COLOR)
        y = cv2.resize(y, (W, H))

        if augment == True:
            # Augmenting data if flag is set to True
            aug = RandomCrop(int(2*H/3), int(2*W/3), always_apply=False, p=1.0)
            augmented = aug(image=x, mask=y)
            x1 = augmented["image"]
            y1 = augmented["mask"]

            aug = HorizontalFlip(always_apply=False, p=1.0)
            augmented = aug(image=x, mask=y)
            x2 = augmented["image"]
            y2 = augmented["mask"]

            aug = VerticalFlip(always_apply=False, p=1.0)
            augmented = aug(image=x, mask=y)
            x3 = augmented["image"]
            y3 = augmented["mask"]

            # Saving augmented images and masks
            save_images = [x, x1, x2, x3]
            save_masks = [y, y1, y2, y3]

        else:
            # If augmentation is not required, keep original images and masks
            save_images = [x]
            save_masks = [y]

        idx = 0
        for i, m in zip(save_images, save_masks):
            i = cv2.resize(i, (W, H))
            m = cv2.resize(m, (W, H))

            tmp_img_name = f"{image_name}_{idx}.{image_extn}"
            tmp_msk_name = f"{mask_name}_{idx}.{mask_extn}"

            image_path = os.path.join(save_path, "images", tmp_img_name)
            mask_path = os.path.join(save_path, "masks", tmp_msk_name)

            cv2.imwrite(image_path, i)
            cv2.imwrite(mask_path, m)

            idx+=1

# Path to the original dataset
path = "/content/dataset/semantic_drone_dataset"

# Getting paths to original images and masks
images = sorted(glob(os.path.join(path, "original_images/*")))
masks = sorted(glob(os.path.join(path, "label_images_semantic/*")))

print(f"Original images:  {len(images)} - Original masks: {len(masks)}")

# Creating directories to save augmented images and masks
create_dir("./new_data/images/")
create_dir("./new_data/masks/")

save_path = "./new_data/"

# Augmenting data
augment_data(images, masks, save_path, augment=True)

images = sorted(glob(os.path.join(save_path, "images/*")))
masks = sorted(glob(os.path.join(save_path, "masks/*")))
print(f"Augmented images:  {len(images)} - Augmented masks: {len(masks)}")

#Creating dataframe
image_path =  os.path.join(save_path, "images/")
label_path = os.path.join(save_path, "masks/")

def create_dataframe(path):
    name = []
    for dirname, _, filenames in os.walk(path):
        for filename in filenames:
            name.append(filename.split('.')[0])

    return pd.DataFrame({'id': name}, index = np.arange(0, len(name)))

df_images = create_dataframe(image_path)
df_masks = create_dataframe(label_path)
print('Total Images: ', len(df_images))
#print(df_images)

#Spliting data

X_trainval, X_test = train_test_split(df_images['id'], test_size=0.1, random_state=19)
X_train, X_val = train_test_split(X_trainval, test_size=0.2, random_state=19)

print(f"Train Size : {len(X_train)} images")
print(f"Val Size   :  {len(X_val)} images")
print(f"Test Size  :  {len(X_test)} images")

# Setting labels (y) to be the same as images for each split
y_train = X_train
y_test = X_test
y_val = X_val

# Creating lists of image and mask paths for each split
img_train = [os.path.join(image_path, f"{name}.jpg") for name in X_train]
mask_train = [os.path.join(label_path, f"{name}.png") for name in y_train]
img_val = [os.path.join(image_path, f"{name}.jpg") for name in X_val]
mask_val = [os.path.join(label_path, f"{name}.png") for name in y_val]
img_test = [os.path.join(image_path, f"{name}.jpg") for name in X_test]
mask_test = [os.path.join(label_path, f"{name}.png") for name in y_test]

# Function to define a U-Net model for semantic segmentation.
# Each convolutional block consists of two convolutional layers followed by batch normalization and ReLU activation.
# The number of filters in each convolutional block is reduced by a factor of 2 using scaling.
# The encoder downsamples the input image through max pooling.
# The bridge connects the encoder and decoder without downsampling.
# The decoder upsamples the feature maps through bilinear upsampling.
# Skip connections concatenate feature maps from the encoder to the decoder.
# The final layer uses a 1x1 convolution to produce the segmentation mask with softmax activation.

def conv_block(inputs, filters, pool=True):
    x = Conv2D(filters, 3, padding='same')(inputs)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Conv2D(filters, 3, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    # If pooling is enabled, apply max pooling
    if pool == True:
        p = MaxPool2D((2,2))(x)
        return x, p
    else:
        return x


def build_unet(shape, num_classes):
    inputs = Input(shape)

    filters_x = [32,64,96,128,128,96,64,32]
    filters_b = [256]

    # Encoder
    x1, p1 = conv_block(inputs, filters_x[0], pool=True)
    x2, p2 = conv_block(p1, filters_x[1], pool=True)
    x3, p3 = conv_block(p2, filters_x[2], pool=True)
    x4, p4 = conv_block(p3, filters_x[3], pool=True)

    # Bridge
    b1 = conv_block(p4, filters_b[0], pool=False)

    # Decoder
    u1 = UpSampling2D((2,2), interpolation='bilinear')(b1)
    c1 = Concatenate()([u1, x4])
    x5 = conv_block(c1, filters_x[4], pool=False)

    u2 = UpSampling2D((2,2), interpolation='bilinear')(x5)
    c2 = Concatenate()([u2, x3])
    x6 = conv_block(c2, filters_x[5], pool=False)

    u3 = UpSampling2D((2,2), interpolation='bilinear')(x6)
    c3 = Concatenate()([u3, x2])
    x7 = conv_block(c3, filters_x[6], pool=False)

    u4 = UpSampling2D((2,2), interpolation='bilinear')(x7)
    c4 = Concatenate()([u4, x1])
    x8 = conv_block(c4, filters_x[7], pool=False)

    # Output Layer
    output = Conv2D(num_classes, 1, padding='same', activation='softmax')(x8)
    return Model(inputs, output)

#Define the resolution of the images and the number of classes

#keeping the original ratio
H = 768
W = 1152
num_classes = 23

model = build_unet((W, H, 3), num_classes)

#Show the summary of the U-Net model and its diagram

model.summary()
#plot_model(model,to_file='model.png')

#Dataset Pipeline used for training the model

# Function to read and preprocess images
def read_image(x):
    x = cv2.imread(x, cv2.IMREAD_COLOR)
    x = cv2.resize(x, (W, H))
    x = x/255.0
    x = x.astype(np.float32)
    return x

# Function to read and preprocess masks
def read_mask(x):
    x = cv2.imread(x, cv2.IMREAD_GRAYSCALE)
    x = cv2.resize(x, (W, H))
    x = x.astype(np.int32)
    return x

# Function to create a TensorFlow dataset from image and mask paths
def tf_dataset(x,y, batch=4):
    dataset = tf.data.Dataset.from_tensor_slices((x,y))
    dataset = dataset.shuffle(buffer_size=500)
    dataset = dataset.map(preprocess)
    dataset = dataset.batch(batch)
    dataset = dataset.repeat()
    dataset = dataset.prefetch(2)
    return dataset

# Function to preprocess images and masks using TensorFlow operations
def preprocess(x,y):
    def f(x,y):
        x = x.decode()
        y = y.decode()
        image = read_image(x)
        mask = read_mask(y)
        return image, mask

    image, mask = tf.numpy_function(f,[x,y],[tf.float32, tf.int32])
    mask = tf.one_hot(mask, num_classes, dtype=tf.int32)
    image.set_shape([H, W, 3])    # In the Images, number of channels = 3.
    mask.set_shape([H, W, num_classes])    # In the Masks, number of channels = number of classes.
    return image, mask

#Training the model

# Seeding
np.random.seed(42)
tf.random.set_seed(42)

# Hyperparameters
shape = (H, W, 3)
num_classes = 23
lr = 1e-4
batch_size = 4
epochs = 30

# Model
model = build_unet(shape, num_classes)
model.compile(loss="categorical_crossentropy", optimizer=tf.keras.optimizers.Adam(lr), metrics=['accuracy'])

train_dataset = tf_dataset(img_train, mask_train, batch = batch_size)
valid_dataset = tf_dataset(img_val, mask_val, batch = batch_size)

train_steps = len(img_train)//batch_size
valid_steps = len(img_val)//batch_size

callbacks = [
    ModelCheckpoint("model.h5", verbose=1, save_best_model=True),
    ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.1, verbose=1, min_lr=1e-6),
    EarlyStopping(monitor='val_loss', patience=5, verbose=1)
]

model.fit(train_dataset,
          steps_per_epoch=train_steps,
          validation_data=valid_dataset,
          validation_steps=valid_steps,
          epochs=epochs,
          callbacks=callbacks
         )

#Prediction
#create the folder for the predictions
create_dir('./results')

# Seeding
np.random.seed(42)
tf.random.set_seed(42)

# Model
model = tf.keras.models.load_model("/content/model.h5")

# Saving the masks
for x, y in tqdm(zip(img_test, mask_test), total=len(img_test)):
    name = x.split("/")[-1]

    #Read image
    x = cv2.imread(x, cv2.IMREAD_COLOR)
    x = cv2.resize(x, (W, H))
    x = x/255.0
    x = x.astype(np.float32)

    #Read mask
    y = cv2.imread(y, cv2.IMREAD_GRAYSCALE)
    y = cv2.resize(y, (W, H))

    y = np.expand_dims(y, axis=-1) #(384,256,1)

    y = y * (255/num_classes)
    y = y.astype(np.int32)
    y = np.concatenate([y, y, y], axis=2)

    #Predict
    p = model.predict(np.expand_dims(x, axis=0))[0]
    p = np.argmax(p, axis=-1)

    p = np.expand_dims(p, axis=-1)

    p = p * (255/num_classes)
    p = p.astype(np.int32)
    p = np.concatenate([p, p, p], axis=2)

    cv2.imwrite(f"./results/{name}", p)

# Iterate through each image and its corresponding mask in the test set.
# Extract the image and mask names from their file paths, specifically looking at the character after the fourth position in the file name.
# If the image name contains '0', indicating it's an original image and not a result of data augmentation, append both the image and its corresponding mask to separate lists.

image_list = []
mask_list = []

for x,y in tqdm(zip(img_test, mask_test), total=len(img_test)):
    name = x.split("/")[-1]
    image_name = name[4]

    name = y.split("/")[-1]
    mask_name = name[4]

    if image_name == '0':
        image_list.append(x)
        mask_list.append(y)

# Visualizing a subset of 7 images along with their corresponding masks and predicted masks to assess the accuracy of the model's predictions.
# Comparing the original image, ground truth mask, and predicted mask to visually inspect the accuracy of the segmentation predictions.

img_selection = image_list[0:7]
mask_selection = mask_list[0:7]

for img, mask in zip(img_selection, mask_selection):
    name = img.split("/")[-1]
    x = cv2.imread(img, cv2.IMREAD_COLOR)
    x = cv2.resize(x, (W, H))

    y = cv2.imread(mask, cv2.IMREAD_GRAYSCALE)
    y = cv2.resize(y, (W, H))


    p = cv2.imread(f"./results/{name}", cv2.IMREAD_GRAYSCALE)
    p = cv2.resize(p, (W, H))

    fig, axs = plt.subplots(1, 3, figsize=(20, 20), constrained_layout=True)

    axs[0].imshow(x, interpolation = 'nearest')
    axs[0].set_title('image')
    axs[0].grid(False)

    axs[1].imshow(y, interpolation = 'nearest')
    axs[1].set_title('mask')
    axs[1].grid(False)

    axs[2].imshow(p)
    axs[2].set_title('prediction')
    axs[2].grid(False)